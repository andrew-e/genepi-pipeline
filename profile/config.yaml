cluster:
  mkdir -p /user/work/$(whoami)/slurm_logs &&
  sbatch
    --partition={resources.partition}
    --account={resources.account_id}
    --nodes={resources.nodes}
    --ntasks-per-node={resources.ntasks_per_node}
    --cpus-per-task={resources.ntasks_per_node}
    --time={resources.time}
    --cpus-per-task={threads}
    --mem={resources.mem}
    --job-name={rule}-{wildcards}
    --output=/user/work/$(whoami)/slurm_logs/{rule}-{wildcards}-%j.out
default-resources:
  - partition=mrcieu
  - mem='1G'
  - account_id=smed001801
  - ntasks_per_node=1
  - nodes=1
restart-times: 3
max-jobs-per-second: 10
max-status-checks-per-second: 1
local-cores: 1
latency-wait: 60
jobs: 500
keep-going: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
